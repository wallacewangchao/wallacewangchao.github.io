<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>project icps</title>

  <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/sub-page.css">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QZ8WNGYXYL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-QZ8WNGYXYL');
  </script>

</head>
<body>
  <main>
    <h1>
        A User Interface for Sense-making of the Reasoning Process while Interacting with Robots
    </h1>
    <p>
        Previously, graph visualization has been used wildly by developers to make sense of knowledge representations. 
        However, due to lacking the link between abstract knowledge of the real-world environment and the robot's actions, traditional visualization tools are incompatible for expert-user to understand, test, supervise and modify the graph-based reasoning system with the embodiment of the robots. 
        Therefore, we developed an interface which enables robotic experts to send commands to the robot in natural language, then interface visualizes the procedures of the robot mapping the command to the functions for querying in the commonsense knowledge database, links the result to the real world instances in a 3D map and demonstrate the execution of the robot from the first-person perspective of the robot. After 3 weeks of usage of the system by robotic experts in their daily development, some feedback was collected, which provides insight for designing such systems.     
    </p>
    <br>
    <br>
    
    <h2>
      Universial Access
    </h2>
    <img src="../img/subpages/kore/scene.png" class="zoom-pic">
    <p>
        A web-based graphical user interface (GUI) is designed and implemented. Expert users can visit the interface anywhere at Honda Research Institute with any device.
        The dialogue box of the interface allows the user to type in commands to the robot in natural language and receive the answer from the robot.
        Users can switch between camera-mode, graph-mode and map-mode.
    </p>
    <br>
    <br>

    <h2>
        User Interface: Camera-mode
    </h2>
    <img src="../img/subpages/kore/cam_mode.png" class="zoom-pic">
    <p>
        The video stream of the first-person view of the robot from the virtual home simulator is displayed. Referred objects from the dialogue box are highlighted with the red stroke in the image.   
    </p>
    <br>
    <br>

    <h2>
        User Interface: Graph-mode
    </h2>
    <img src="../img/subpages/kore/graph_mode.png" class="zoom-pic">
    <p>
        The graph mode visualizes the called sequence of functions and corresponding input/output.
        First level function can expand into a hull and show detailed information in the knowledge graph.
        People can select and highlight the links for inspection. Furthermore, by pressing the "delete" button on the keyboard, the highlighted links will be deleted from the database.     
    </p>
    <br>
    <br>

    <h2>
        User Interface: Map-mode
    </h2>
    <img src="../img/subpages/kore/map_mode.png" class="zoom-pic">
    <p>
        User can zoom in/out and rotate the view to observe the map and the 3D graph.
        Above the map, a 3D graph visualization is provided.    
    </p>
    <br>
    <br>

    <h2>
      Publications:
    </h2>
    <a href="https://arxiv.org/abs/2210.08246" target="_blank">
        Wang, C., & Deigmoeller, J. (2022). A User Interface for Sense-making of the Reasoning Process while Interacting with Robots. arXiv preprint arXiv:2210.08246.
    </a>

  </main>

</body>
</html>